# 吴恩达机器学习笔记

> From Felix Du , in Guangdong Ocean University ;
>
> 写在前面的寄语：**“很期待下次见面，到那时候我们都会变得更好！”**

------

[TOC]

## 1.1 欢迎来到机器学习

##### 生活中的机器学习

- Google搜索引擎
- 手机相册的人脸分类
- 软件的推荐算法
- 语音转文字
- Siri等等......

##### 工业上的一些应用

- 风力涡轮发电机的优化
- 医学影像智能识别等等......

------



## 1.2 机器学习的应用

> 机器学习属于人工智能的子领域，让机器帮人脑解决一些依赖大量数据和抽象信息的事情

```cpp
char AGI：人工通用智能
```

------



## 2.1 什么是机器学习

### 1、定义

> “Field of study that gives computers the ability to learn without being explicitly programmed."  ——from Arthur Samuel（1959）

​	机器学习就是计算机在**没有明确编程的情况下学习**的研究领域。

### 2、机器学习的类型

​	机器学习算法有两种主要类型，分别是**监督学习**和**非监督学习**；（监督学习用得比较多）

​	其他的算法：强化学习、半监督学习等......

- 学习目标：能够自己构建一个机器学习系统

------



## 2.2 监督学习Part1（Regression）

### 1、定义

​	监督学习（Supervised learning）：指的是学习x到y，或者输入到输出映射的算法；机器学习的基本特征是，提供算法示例以供学习，让机器知道x到y的的关系是怎样的，从而对一些事情进行预测与推理。

**一些监督学习算法的应用**

![image-20250422012331464](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250422012331464.png)

##### 例子：房价预测：

![image-20250422012522312](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250422012522312.png)

> 不同的拟合方式，模型的效果会有所不同，比如拟合成直线在这个情况下，就不如拟合成曲线

​	当然在这个例子中，机器学习做的就是拟合一个最接近实际的直线或曲线；这中特殊类型的监督学习有一个专门的术语，称为**回归**

##### 	**回归（Regression）**：从无数可能的数字中预测一个数字。

​	例如上面的房价。

------



## 2.3 监督学习Part2（Classification）

##### 第二种监督学习算法：分类（classification）

​	分类方式就是一个预测分类的算法，当然这个分类不一定是数字，还可以是标签，例如cat、dog......

​	与回归不同的是，我们只需要试图预测一小部分可能的输出或类别（例如是否患病）

​	注意：在分类算法当中，输出类（class）和输出类别（category）经常互换使用。

##### 如果是多个输入值（input）呢？

​	例如从年龄和肿瘤尺寸推测这是良性或者恶性的。

![image-20250422013857910](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250422013857910.png)

​	学习算法要做的就是找到一个边界来区分恶性肿瘤和良性肿瘤，算法必须决定如何将边界线拟合到这些数据上。在一些机器学习例子中，往往需要用到更多的输入，比如这个例子的实际应用中，往往会用到例如肿瘤的厚度，细胞均匀度等指标来衡量......

------



## 2.4 非监督学习 Part1

### 1、定义

​	无监督学习（unsupervised learning）：

​	在分类问题中，无监督学习并不会给出x到y映射的数据集，只会给出诸如肿瘤尺寸以及年龄的数据，并不会给你y的输出，例如该肿瘤是否良性。

> 可以理解为无监督学习就是没有输出，而是完成**聚类**

![image-20250422093723732](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250422093723732.png)

#### 无监督学习类型1：聚类算法（clustering）

​	将未标记的数据放入不同的集群中；

​	准确点说，就是机器通过分析，将相似的数据点组合在一起，这就叫聚类。

> 例如Googe news，就是每天查看互联网上数十万篇新闻文章，找到相似词的文章并将他们分组到集群中，将相关故事组合在一起；

**示例二**：DNA microarray

![image-20250422094454188](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250422094454188.png)

​	这是一个人的DNA，其中一个区域可以看作表达某种形状的基因；在此图中，红色、绿色、灰色等这些颜色显示了不同个体具有或者不具有特定基因活性的程度。

​	我们可以通过聚类算法，将不同个体放到一个聚群里面；例如大部分绿色，我们分为type1、绿色+红色，分为type2，全红分为type3......

------



## 2.5 非监督学习 Part2

> 在此部分学习另外两种非监督学习算法。

### 异常检测（Anomaly detection）

> 用于检测异常事件，例如在金融系统中的欺诈检测等.....

### 降维技术（Dimensionality reduction）

> 将大数据压缩成一个小得多的数据集，同时丢失尽可能少的信息。

 

------



## 3.1 线性回归模型 

### 1、定义

> 线性回归模型（Linear Regression Model）：意味着将一条直线拟合到你的数据中，这个模型是当今世界上应用最广泛的学习算法。

### 2、示例：根据房屋的大小预测价格

> 原先的数据集放到图中，然后用一条直线拟合的结果如下：

![image-20250425010921734](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250425010921734.png)

​	据此，我们就可以预测处1250英尺的房子大概值多少钱了。

​	**线性回归模型是一种特殊类型的监督学习模型。称为回归模型**

​	Tips：在回归模型当中，输出的数据是**无限多的**，这也是回归模型区分与另一种监督学习算法——分类模型的重要标志。

> 也可以用一个数据表来表示（左侧是输入：房子大小；右侧是输出：房子价格）

![image-20250425011338354](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250425011338354.png)

### 3、相关术语与符号

- 训练集：用于训练模型的数据集；
- 输入变量(input variable)：**标准符号是$x$**，一个小写的X，我们称为输入变量，也称为特征(feature)或输入特征，例如对于训练集中的第一个房子，$x$是房子的大小，因此$x=2104$
- 输出变量(output variable)：**标准符号是$y$**，一个小写的Y，我们称为输出变量，又是也称为目标变量(target variable)，例如在训练集中，表示的是房子的价格，因此$y=400$
- 训练示例的总数（number of training examples） ：**用小写的$m$表示**，表示训练的数据有多少对。
- 单个训练示例（single training example）：用他的输入变量和输出变量$(x,y)$组合表示。
- 表示特定的示例：用$(x^{i},y^{i})$表示第$i$个这个特定的训练示例；这里的i不是求幂，只是一个索引；

### 4、监督学习的流程

![image-20250425195413791](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250425195413791.png)

**如何表示函数线性回归模型 f **：

​	可以表示为**$f_{x,b}(x)=wx+b$**，这个公式表示$f$是一个以$x$为输入的函数，并且根据$w$和$b$的值，$f$将输出预测 y-hat 的某个值；有时也可以用$f(x)$表示$f_{w,b}(x)$

------



## 3.3 代价函数

### 1、代价函数可以干嘛

> 代价函数将告诉我们模型的运行情况，以便我们可以尝试让它做的更好。

例如：在线性回归模型当中，你的模型表示为：$f_{x,b}(x)=wx+b$，其中w、b是这个模型的参数（parameters），我们可以调整w、b的值来改进模型，有时候我们也称为系数（coefficients）或权重（weights）

**感受以下w、b是如何改变直线的走向的**

![image-20250425200852380](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250425200852380.png)

> 所以：对于线性回归，我们要做的就是选择参数w和b的值，以便我们从函数 f 获得的直线以某种方式很好地拟合数据。

### 2、什么是代价函数

​	在线性回归中，我们常用以下的公式来衡量我们预测的误差：$\frac{\sum_{i=1}^{m}(\hat{y}^{i}-y^{i})^{2}}{m}$其中$\hat{y}^{i}-y^{i}$表示的是误差error，m表示数据集的数量；其实就是表示的平均误差；

​	按照惯例，在机器学习中，我们常是除以2m而非m，这样会使得我们后面的运算更加简便；即：$J(x,b)=\frac{\sum_{i=1}^{m}(\hat{y}^{i}-y^{i})^{2}}{2m}$，但是无论是否是除以2m，这个成本函数依然有效。

> 在机器学习中，不同的人会针对不同的模型使用不同的代价函数，但是平方误差成本函数是迄今为止线性回归最常用的函数。

​	对于，模型为$f_{x,b}(x)=wx+b$的函数，我们可以用$J(w,b)=\frac{\sum_{i=1}^{m}(f_{w,b}(x^{i})-y^{i})^{2}}{2m}$来表示它的代价函数。

### 3、代价函数的直观理解

- model：$f_{w,b}(x)=wx=b$
- parameters：$w,b$
- cost function：$J(w,b)=\frac{\sum_{i=1}^{m}(f_{w,b}(x^{i})-y^{i})^{2}}{2m}$
- goal：$min _{w,b}J(w,b)$找到最适合的w，b使得代价函数的值最小。

简化线性回归模型的可视化图（令b=0）

![image-20250426163007968](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250426163007968.png)

**线性回归的目标就是找到参数w或w和b，使代价函数 J 的值最小**

### 4、线性回归模型代价函数的可视化

![image-20250426163915173](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250426163915173.png)

- 改变w、b的值，会使纵轴 J 的值发生改变。J 的最小值就在这个三维图像的底部，像地理的等高线，最小值在同心圆的最中心。

------



## 4.1 梯度下降

> 一种更系统的方法来找到w和b使得代价函数最小，梯度下降在其他机器学习算法中无处不在，不仅用于线性回归，还用在一些最先进的神经网络模型，也称为深度学习模型。

### 1、什么是梯度下降

- Have some function $J(w,b)$
- Want $min_{w,b}J(w,b)$

​	梯度下降是一种可用于尝试最小化任何函数的算法，而不仅仅是线性回归的代价函数。

##### 梯度下降怎么实现

1、从w和b的一些初始猜测开始（线性回归中，初始值是多少并不重要，所以一个常见的选择是都设置为0）

2、每次都稍微改变参数w和b以尝试降低w和b的成本 j 

3、直到希望 j 稳定在或接近最小值。

**注意：**对于某些可能不是弓形或吊床形的函数 j ，可能存在不只一个可能的最小值。

我们要做的就是不断环绕360度，然后走一小步，直到到达最低点。

![image-20250427155351395](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250427155351395.png)

如果你站在一个不同的起始点：

![image-20250427155457812](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250427155457812.png)

**其中，第一个和第二个谷底都称为局部极小值。**

------



## 4.2 实现梯度下降

> 本节课，用来研究看可以用哪些数学表达式来实现梯度下降起作用。

#### 1、对参数$\omega$的实现

$\omega=\omega-\alpha\frac{\partial}{\partial{\omega}}J(w,b)$,即为w的旧值减去$\alpha$乘以wb的代价函数 J 对w的偏导数

- 在这里的等号是赋值运算符，即将新的$\omega$的值赋给前一个$\omega$
- 在上面的公式中，$\alpha$也被称为学习率（Learning rate），学习率通常是0到1之间的一个小正数。$\alpha$的作用是控制下坡的基本步幅，如果$\alpha$特别大，那么说明函数在急剧地下坡；如果$\alpha$非常小，则说明是一步一步地小心下坡。
- 关于代价函数的偏导：$\frac{\partial}{\partial{\omega}}J(w,b)$是为了告诉我们要朝哪个方向迈出梯度下降的步伐。

#### 2、对参数$b$的实现

> 我们的代价函数中不只有$\omega$一个参数，还有b，b的实现公式如下：

$b=b-\alpha\frac{\partial}{\partial{b}}J(w,b)$

关于这个公式的一些解释同上面$\omega$的公式类似，在此不多赘述。

#### 3、实现同时更新的梯度下降的正确方法

> 也就是同时对$\omega$以及$b$两个参数进行更新的方法。

​	$tmp_{\omega}=\omega-\alpha\frac{\partial}{\partial{\omega}}J(w,b)$

​	$tmp_b=b-\alpha\frac{\partial}{\partial{b}}J(w,b)$

​	$\omega=tmp_{\omega}$

​	$b=tmp_b$

这样子求$\omega$和$b$时的参数$\omega$和b就是相同的

如果是下面这种方式：

​	$tmp_{\omega}=\omega-\alpha\frac{\partial}{\partial{\omega}}J(w,b)$

​	$\omega=tmp_{\omega}$

​	$tmp_b=b-\alpha\frac{\partial}{\partial{b}}J(w,b)$

​	$b=tmp_b$

那么当我们在求b的时候，代价函数中的参数$\omega$就变成了我们刚刚更新的参数，所以是错误的。

------



## 4.3 学习率

> 学习率$\alpha$的选择将对我们实现梯度下降的效率产生巨大的影响，如果学习率选择不当，梯度下降可能不起作用。

### 1、学习率太小

- 如果学习率太小，虽然也是在不断地趋近最小值，但是要走很多步，速度太慢了。

### 2、学习率太大

- 如果学习率太大，梯度下降的结果可能会超过，并且可能永远不会达到最小值。
- 换句话说，梯度下降可能无法收敛，甚至可能发散

### 3、一种特殊情况

![image-20250430142831982](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250430142831982.png)

如果代价函数的参数$\omega$现在处于函数的一个局部最小值，这个时候的偏导恒等于0，也就是说$\omega$的取值不会发生任何改变。

> 也就是说，当你的参数已经达到了局部最小值，那么进一步的梯度下降步骤将完全没有作用。
>
> 这也解释了：为什么梯度下降即使用固定的学习率$\alpha$也能达到局部最小值

![image-20250430143604436](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250430143604436.png)

我们可以看到，越接近代价函数最小值，图像的斜率越来越小，那么导数也会越来越小，这就导致我们走的每一步都越来越小（即使我们采用一个恒定的学习率），这就是梯度下降算法。

------



## 4.4 运行梯度下降

![image-20250502185223807](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250502185223807.png)

- 准确来说，这种梯度下降算法叫做批量梯度下降。批量梯度下降着术语指的是**在梯度下降的每一步中，我们查看的是所有训练样本，而不仅仅是训练数据的一个子集**
- 此外，还有其他版本的梯度下降不会查看整个数据集，而是在每个更新步骤查看训练数据的较小子集。
- 不过我们还是习惯用批量梯度下降来进行线性回归

### 关于代码练习：看jupyter notebook

------



## 5.1 多类特征

> 学习如何让线性回归更强大，先来看一个不仅关注一个特征的线性回归版本

在预测房屋价格的例子中，我们现在不只是知道房子的大小，我们还知道其他参数，比如多少层楼、多少件卧室等等....

![image-20250504163543232](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250504163543232.png)

- 有时候我们会给$x_2$上面加一个箭头，表示这不是一个数，是一个向量

- 下面让我们来看看模型会长什么样子

> 因为我们现在不只是关注$\omega$一个参数，我们还有其他的更多参数，所以我们这样表示

$f_{w,b}(x)=\omega_{1}x_1+\omega_{2}x_2+\omega_{3}x_3+\omega_4x_4+b$

我们可以把系数$\omega$理解成权重：重写$f_{\vec{w},b}(x)=\vec{w}.\vec{x}+b=w_1x_1+w_2x_2+...+w_nx_n+b$

- 其中这个点积是代表两个数字列表的两个向量，通过对所取对应的数字对逐个相乘

，点积符号能够让我们的表达式变得更加简便。

- 所以如上：拥有多个输入特征的线性回归模型我们称为多元线性回归模型，这与单变量线性回归形成对比。
- 为了实现我们多元线性回归的目的，我们用了一个好用的方法：向量化，这将使得实现这一点以及许多其他学习算法变得更加简单。

------



## 5.2 向量化

> 为什么要向量化？因为在我们使用机器学习的过程中，使用向量化不仅会让你的代码更简介，还会让它运行更高效。
>
> 学会如何编写向量化代码还可以让我们利用现代数值线性代数库，甚至是GPU硬件

### 1、什么是向量化？

​	先看下面这一个例子：

​	对于线性回归模型我们有多个输入特征（即不同的$\omega$，这个时候我们可以用NumPy库来实现我们的目标：

```python
import NumPy
w = np.array([1.0,2.5,-3.3])
b = 4
x = np.array([10,20,30])
#注意：在Python或者其他主流语言中，数组的下标都是从0开始的
```

​	如果我们不使用向量来计算的话：

​	我们的代码就得这样写：

```python
f = w[0] * x[0] +
	w[1] * x[1] +
    w[2] * x[2] + b
```

​	当然，你的数组比较短的时候还挺方便，但是如果你的数组规模比较大，n=10000呢？

​	其实也可以不用向量化来完成代码，可以采用for循环

```python
f = 0
for j in range(0,n):
    f = f + w[j] * x[j]
f = f + b
```

​	那么当我们使用向量化之后呢，我们的代码会非常简介，用一句话就可以搞定：

```python
f = np.dot(w,x) + b
#实现了向量w和x之间的数学点积
#dot函数表示的是点积
```

​	上面的代码，在计算中，尤其是n极大时，会十分方便快捷。

### 2、向量化的好处

- 1、使得代码更简介
- 2、使代码实现的速度更快（因为NumPy的dot函数能够利用电脑中的并行硬件）

### 3、向量化是怎么让我们代码的运行速度变快的

​	![image-20250505035648417](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250505035648417.png)

​	如果向量化的话，电脑会获得关于w和x数组的所有值，在每个步骤中，并行地将每一对w和x相乘，然后计算机将这16个数字取出并使用专用硬件将他们全部相加，而不是一个一个地进行不同的加法来加总这16个数字。

​	这意味着带有向量化的代码可以在更短的时间内完成计算，而没有向量化的代码则需要更长的时间。之所以要实现向量化，是因为一切的模型训练都要基于大量的数据集，所以效率对于我们来说尤为重要。

**补充一点：NumPy在小规模的时候并不会启用多线程，只有在大规模数据时才会并行处理**

------



## 5.3 多元线性回归的梯度下降法

### 1、向量化代价函数

​	本质上和原先向量化多元线性回归模型一致，向量化如下

> 原先的代价函数：$J(w_1,...,w_n,b)$
>
> 经过向量化之后（知识把w参数进行向量化）：$J(\vec{w},b)$

![image-20250505045430729](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250505045430729.png)

​	我们将反复更新的每个参数$w_j$为$w_j$ 减去$\alpha$乘以代价函数 j 的偏导；其中左侧为没有向量化的表达，右侧为向量化之后的表达。

​	![image-20250505045921700](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250505045921700.png)

​	多元线性回归的梯度下降法与单变量线性回归的梯度下降法有以下的两个不同:

- 一是输入特征数据$\omega$从一个数，变成了一个向量$\vec{\omega}$
- 而是多元线性回归有从1到 n 多个输入数据$\omega$，所以要有多次的梯度下降来更新参数；

### 2、正规方程（Normal equation）

> 这是一个用于寻找线性回归的w和b的替代方法
>
> 这个方法同样使用与线性回归，但是仅使用与线性回归，而不适用于机器学习领域的其他算法。
>
> 这种方法不需要迭代的梯度下降算法，并且可以使用高级的线性代数库来一次性求解w和b而无需迭代
>
> 但是也有不少缺点：
>
> 1、只能用于线性规划
>
> 2、如果特征数量n很大，正规方程方法的求解会很慢

**注意：我们很少自己亲自用正规方程来求解w和b；我们一般都是在后台，通过调用一些库来帮助我们实现的**

但是对于我们常常碰到的其他模型算法，梯度下降法往往是更好的方法；

------

 

## 6.1 特征缩放

### 3、什么是特征缩放

> 这是一种让梯度下降运行得更快的技术：在不同的特征取值范围差异很大时，这可能导致梯度下降运行缓慢，但是重新缩放不同的特征，使它们都在相似的范围内取值，可以加快梯度下降的速度。

先看下面这个例子

![image-20250505161122225](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250505161122225.png)

​	我们注意到，当一个特征的可能取值范围很大时，例如房子的大小，那么参数的值就不能太大，例如我们选择的是0.1

​	与梯度下降的关系是什么呢？我们看一下特征的散点图

![image-20250505161908390](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250505161908390.png)

​	在这等高线图中，在一侧较短而在另一侧较长，这是因为$w_1$的一个很小的变化就可以对估价产生很大的影响，这是因为$w_1$乘的是一个很大的系数。

​	我们可以简单地记为：**大的配小系数，小的配大系数**

​	其实就是数据归一化，使得能够在一个[0,1]区间中。这样梯度下降可以找到一条更直接的路径到达全局最小值。

![image-20250505162225629](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250505162225629.png)

### 2、如何使用特征缩放

#### 1）特征缩放方法一：除以最大值

![image-20250505163250071](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250505163250071.png)

#### 2）方法二：进行均值归一化

> 把原始数据缩放到一个x属于[-1,1]，y也属于[1,1]的区间，从原先都大于0的数据缩放成带有正负的数据。

​	1、先找到原始特征的平均值，我们记作$\mu_1$

​	2、然后用每个特征$x_n$来减去$\mu_1$，然后除以范围的最大值减最小值

​	Q：之所以要减去平均值，是为了让缩放后的数据均匀地分布在四个象限内

![image-20250505163836533](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250505163836533.png)

#### 3）Z-score 归一化

​	步骤如下：

​	1、计算出均值$\mu$和标准差$\sigma$

​	2、然后取每个特征$x_n$来减去$\mu$，然后除以标准差$\sigma$

> 有点像是标准正态化

![image-20250505164338111](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250505164338111.png)

### 3、特征缩放的一些经验

![image-20250505164705407](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250505164705407.png)

​	特征缩放几乎没有坏处，所以我们鼓励进行特征缩放，这会让我们梯度下降的速度更块。

------



## 6.2 检查梯度下降是否收敛

### 1、什么是梯度下降是否收敛

![image-20250505165449193](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250505165449193.png)

​	这是一个梯度下降的学习曲线，其中表示随着横轴迭代次数的增加，代价函数 J 的值的变化趋势，我们可以看到，当我们迭代到300次之后，代价函数 J 的值并没有明显的下降，所以我们认为梯度下降或多或少已经收敛。

​	所以通过看学习曲线，我们可以尝试判断梯度下降是否正在收敛。

​	**注意：梯度下降收敛所需的迭代次数在不同的情况下可能会有很大的差异**

​	**所以我们可以通过判断是否收敛来决定模型何时完成训练。**	

​	另一种决定模型何时完成训练的方法是使用自动收敛测试：

​	设置一个极小的变量$\epsilon$,如果代价函数$J(\vec{w},b)$小于$\epsilon$，则说明收敛了

------



## 6.3 学习率的选择

> 学习率的选择至关重要，如果学习率太小，运行会十分缓慢；如果太大，可能不会收敛。

### 1、如何选择合适的学习率

​	当我们发现随着迭代次数的增加，代价函数的大小并不呈现收敛，一般有以下两种可能：

​	1、代码错误

​	2、学习率$\alpha$太大（我们本节主要为了解决这个问题）

![image-20250506204323416](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250506204323416.png)

​	那么我们在选择合适的学习率的时候，通常会尝试一系列的学习率$\alpha$的值，知道找到一个偏小的学习率与偏大的学习率，然后不断在这个区间里面去接近我们最合适的学习率。

![image-20250506204555153](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250506204555153.png)

------



## 6.4 特征工程

> 我们数据集当中可能会有很多特征，但在很多应用当中，选择和设计正确的特征是使算法良好运行的关键步骤。所以如何选择或设计最合适的特征用于学习至关重要。

### 1、什么是特征工程

​	所谓特征工程，就是我们利用对问题的知识或直觉来设计新的特征，通常是通过转化或组合问题的原始特征，以便让学习算法更容易做出准确的预测。

​	例如在以下例子中，我们就创建了一个新的特征$x_3$作为房屋面积进行训练：

![image-20250506205254198](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250506205254198.png)

------



## 6.5 多项式回归

> 先前我们只是对数据进行直线拟合，本节课我们将结合多元线性回归和特征工程的概念，提出一种新的算法，称为多项式回归；这个算法可以让我们去拟合曲线、非线性函数到我们的数据中。

​	以下是多项式回归的几个例子：

![image-20250506205720363](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250506205720363.png)

​	此外，我们可以将特征提升到2次方、3次方或其他任何次方（包括平方根）。

​	因为我们对特征进行了幂次的处理，所以进行特征缩放尤为重要。

------



## 7.1 Motivations

> 这一节只是让我们思考一下，为什么线性回归不适合分类模型？从而引入我们新的算法，逻辑回归算法，这是分类模型中使用最广泛的算法。

​	在几个常见的分类问题中，我们通常只要输出yes或者是no这两个结果，这种分类问题我们称为二元分类，其中二元这个词指的是只有两个可能的类别；把0称为消极分类（negative class）把1称为积极分类（positive class）

![image-20250507222849242](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250507222849242.png)

​	negative和positive并不分别表示坏或者好，只是负例和正例用于传达缺失的概念。

​	Q：为什么不适合线性回归？

​	预测的值太多了，不只是0和1，还有0.5、0.6，这会导致类别太多了...

![image-20250507223222622](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250507223222622.png)

​	当然我们可以像上面一样，用一个范围来表示0和1；这也能实现简单分类；但是在现实生活中，我们的数据集不可能这么完美，比如我们突然多了一个训练数据：

![image-20250507223444564](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250507223444564.png)

​	那么我们拟合的直线会发生改变，这会导致原先恶心的肿瘤被划分为良性，这是一个很糟糕的分类结果。我们看到了线性回归导致了最佳拟合线发生便宜，从而使得我们的决策边界也发生偏移。这也就是我们不用线性回归来解决分类问题的原因；

------



## 7.2 逻辑回归（logistic）

### 1、sigmoid function（logistic function）	

​	同样用判断肿瘤良性or恶性为例子：

​	![image-20250508220342431](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250508220342431.png)

​	我们会建立sigmoid function（有时也称logistic function）逻辑函数，表达式如下：

​	$g(z)=\frac{1}{1+e^{-z}},0<g(z)<1$

​	当z是一个非常大的正数时，分母会很接近1，这就导致g(z)的值很接近于1；

​	同样当z是一个非常大的负数（指的是接近$-\infty$时，分母特别大，接近$+\infty$，这就导致g(z)的值很接近于0

​	此外，当z=0时，分母等于2，所以y=0.5；

### 2、逐步建立逻辑回归算法

1、一个直线（如线性回归中出现的）

​	$f_{\vec{w},b(x)}$

​	$z =\vec{w}.\vec{x}+b$

2、然后把z的值传到sigmoid function中

​	$g(z)=\frac{1}{1+e^{-z}}$

3、把这两个式子组合在一次，就会得到逻辑回归函数

​	$f_{\vec{w},b}(\vec{x})=g(\vec{w}.\vec{x}+b)$

​	其中$\vec{w}.\vec{x}+b$也可以用$z$表示：

​	$f_{\vec{w},b}(\vec{x})=g(\vec{w}.\vec{x}+b)=g(z)=\frac{1}{1+e^{-(\vec{w}.\vec{x}+b)}}$

​	这就是逻辑回归模型，他的作用就是**输入一个特征或者一组特征x，输出一个在0和1之间的数值**

### 3、如何解释逻辑回归的输出

- 可以解释为输出值为1的概率，例如输出结果为0.7，那么我们就可以认为这个患者的肿瘤是恶性的概率是70%，所以有时候在看到一些关于逻辑回归模型的论文或博客时，会让

​	$f_{\vec{w},b}(\vec{x})=p(1)$，也就是在输入数据为x的情况下，输出1的概率

------



## 7.3 决策边界

> 用来帮助逻辑回归算法计算其预测值

![image-20250508223349785](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250508223349785.png)

​	看一个例子，有两个输入数据，其中训练集中红叉表示正例（y=1）、圆圈表示负例（y=0）

![image-20250508223651043](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250508223651043.png)

​	经过拟合，确定出$w_1=1;w_2=1;b=3$拟合出一条直线（图中紫色），这就是决策边界，当在这条上的右上方时，y=1，在左下方时y=0；

​	一个更复杂的例子，决策边界不再是一条直线；

![image-20250508224022405](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250508224022405.png)

​	当$x_1^{2}+x_2^{2}\geq1$时，预测值y=1；

​	当$x_1^{2}+x_2^{2}\leq1$时，预测值y=0；

​	我们也可以提出更复杂的决策边界（通过特征工程和多项式特征来实现）：

![image-20250508224338634](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250508224338634.png)

------



## 8.1 逻辑回归的代价函数

> 为什么平方误差成本函数不是逻辑回归的理想成本函数？

如果我们使用同样的代价函数来计算逻辑回归方程，他的图像将不再是一个凸函数，这意味着我们尝试使用梯度下降法的时候，会在很多局部最小值中陷入困境。

![image-20250519221622905](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250519221622905.png)

我们要做的就是找到一个新的代价函数，让代价函数的图像凸起，这样梯度下降才能保证收敛到全局最小值。

我们定义出以下的代价函数，下方表示的是y=1时的图像

![image-20250519222255533](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250519222255533.png)

以下是y=0时的图像

![image-20250519222437321](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250519222437321.png)

![image-20250519222521881](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250519222521881.png)

------



## 8.2 逻辑回归的简化版代价函数

​	因为我们的损失函数中，y的取值只有1和0，所以可以写出以下简化版本

![image-20250520205702755](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250520205702755.png)

​	为什么可以写成下面的形式呢？

​	y只能取1或者0，在第一种情况下，我们假设y=1：

![image-20250520210314301](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250520210314301.png)

​	我们可以看到后面这一大项全部消失了，也就是我们上面所写的$-log(f_{\vec{w},b}(\vec{x}^{(i)}))$

​	当y=0 时，前面的一整部分就会被消掉，那么损失函数还是等于$-log(1-f_{\vec{w,b}}(\vec{x}^{(i)})$

​	这样就实现了我们对损失函数简化的目标；

​	那么这样子我们的代价函数也会得到相应的简化

![image-20250520210952138](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250520210952138.png)

​	这个代价函数推导来的背后原理是概率论中的最大似然估计，了解一下即可。

------



## 9.1 逻辑回归梯度下降的实现

> 解决一个问题：如何找到合适的参数w和b

常见的代价函数以及确定w和b值的梯度下降法

![image-20250521115154855](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250521115154855.png)

​	我们可以经过一定的变形转换，得到下面的表达式

![image-20250521115407383](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250521115407383.png)

​	需要注意的是，这里的参数w和b和先前我们线性规划中一样，是同时更新的（simultaneous updates）

​	虽然我们发现这个公式和线性回归的公式是一致的，但是这并不表示线性回归和逻辑回归的算法是一样的，事实上这是两个非常不同的算法，因为他们对于$f(x)$的定义不同（如下）

![image-20250521115925192](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250521115925192.png)

------



## 10.1 过拟合的问题

### 1、什么是过拟合（overfitting）

#### 	1）线性回归的过拟合：

![image-20250526200446547](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250526200446547.png)

- 第一个算法无法很好地拟合数据，对此的专业术语是欠拟合（另一个是说算法有高度偏差（high bias），这组数据中有一个明显的模式（或者预设）但是我们的算法却无法很好地拟合他。所以这种认为数据是线性的预设使它拟合来一条与数据不符的直线。（underfit）
- 第二个例子，我们用多个特征来对数据进行拟合，得到的结果与我们的数据拟合度比较高，在用来预测房屋价格的时候更有参考价值，所以我们说这个学习算法能够很好地泛化。
- 如果我们拟合一个四阶多项式，我们看似对原先的数据的拟合程度特别好，而且如果我们去计算代价函数，误差甚至等于0；但是这并不表示我们的算法很好，因为他对预测房子价格的参考意义不大，缺乏范式；这就叫过拟合（overfit：拟合得太过完美了）另一种说法是我们的算法有高方差（high variance）

> Tips：在机器学习中，欠拟合 == 高度偏差 ；过拟合 == 高方差

​	机器学习的目的就是找到一个恰到好处的模型，既不欠拟合，也不过拟合；

#### 	2）分类（逻辑回归）中的过拟合

![image-20250526201054688](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250526201054688.png)

​	需要注意的是，我们不喜欢过拟合不是因为他拟合得不好，相反，正是因为他拟合得太过完美了，会使得图像的方差很大，这就会导致我们的预测失准；不适合推广到新样本。

------



## 10.2  解决过拟合

### 1、方法一：收集更多的训练数据

​	这样子拟合图像的摆动就不会幅度特别大特别离谱

![image-20250526201436871](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250526201436871.png)

### 2、拟合特征筛选（直接令参数=0）

​	当你的特征过多，同时你的训练数据又不足的时候，你就会出现过拟合的问题；因此，我们可以不用所有的特征来拟合，而是选取特征集合中的一个子集，这个子集是最有代表性的特征，用这个较小的特征子集，就会使得拟合效果更好；

![image-20250526201808355](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250526201808355.png)

### 3、正则化（regularization）

​	当我们找出要删掉的特征，只需要让特征前的参数等于0，就是去掉这个特征；而正则化，用一种更温和地减少某些特征影响的方法，而不需要去除它们；

​	正则化所做的是鼓励学习算法缩小参数值，而不必要求参数精确到0；正则化可以让你保留所有的特征，只是防止特征产生过大的影响；

![image-20250526202135568](C:\Users\Duuuzx\AppData\Roaming\Typora\typora-user-images\image-20250526202135568.png)

​	通常，按照惯例，我们一般只缩小$w_{j}$的参数大小，是否正则化参数b对结果没有很大的影响

------



## 10.3 正则化代价函数
